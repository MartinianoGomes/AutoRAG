# Configuração AutoRAG com Modelos LOCAIS (HuggingFace)
#
# Esta configuração usa modelos do HuggingFace que rodam localmente.
# NÃO REQUER GPU - funciona com CPU (mais lento)
# NÃO REQUER API KEY - 100% gratuito
#
# AVISO: O primeiro uso vai baixar os modelos (~500MB a 2GB)
#
# EXECUÇÃO:
# autorag evaluate --config tutorial/config_local_huggingface.yaml \
#   --qa_data_path tests/resources/qa_data_sample.parquet \
#   --corpus_data_path tests/resources/corpus_data_sample.parquet \
#   --project_dir tutorial/projeto_local_hf

# VectorDB em memória com embedding HuggingFace local
vectordb:
  - name: chroma_hf
    db_type: chroma
    client_type: ephemeral
    embedding_model:
      - type: huggingface
        model_name: sentence-transformers/all-MiniLM-L6-v2
    collection_name: autorag_hf_local

node_lines:
  # 1. Linha de Retrieval
  - node_line_name: retrieve_node_line
    nodes:
      # Busca Léxica com BM25
      - node_type: lexical_retrieval
        strategy:
          metrics: [retrieval_f1, retrieval_recall, retrieval_precision]
        top_k: 3
        modules:
          - module_type: bm25
            bm25_tokenizer: [porter_stemmer, space]

      # Busca Semântica com embedding HuggingFace
      - node_type: semantic_retrieval
        strategy:
          metrics: [retrieval_f1, retrieval_recall, retrieval_precision]
        top_k: 3
        modules:
          - module_type: vectordb
            vectordb: chroma_hf

  # 2. Linha de Geração (apenas prompt, sem LLM)
  # NOTA: Geração com HuggingFace local requer GPU ou muito tempo de CPU
  - node_line_name: post_retrieve_node_line
    nodes:
      # Construtor de Prompt - retorna o contexto formatado
      - node_type: prompt_maker
        strategy:
          metrics:
            - metric_name: bleu
        modules:
          - module_type: fstring
            prompt: |
              Contexto recuperado do banco de dados:
              {retrieved_contents}

              Pergunta do usuário: {query}

              Resposta baseada no contexto acima:
